31.01.2020
Broad topic:  definition of biases expressed in language.  Two main perspectives:
• philosophical:  concepts are characterised by properties.  Properties can be
necessary (definitional) or additional.  A biased concept differs from a neu-
tral one because of the different configuration of non-necessary properties.
Let’s explore how these properties change.
• distributional:  biased concepts can be identified by their geometrical rep-
resentation in the semantic space. We can look for a geometrical/operative
definition that all (or the majority) of biased concepts can share.

Proposal:  take  two  polarised  communities  from  Reddit,  build  two  semantic
spaces and compare them, focusing on some target concepts.  It can be hard to
compare concepts coming from different semantic spaces since all concepts are
connected and perturbations of a concept would result in perturbations of the
semantic space as a whole.  An idea could be to use rotation/scaling at a global
or at a local level in both the semantic spaces (and the code Aurelie provided)
to check how different they are whether it makes sense to compare them.  If the
experiment fails, we can analyse the reasons for this failure.

!   Idea:  use  also  the  information  you  can  extract  about  speakers  since  lan-
guage is strongly influenced by the identity of who produces it (Emily Bender
workshop).

To do
•define some target concepts (e.g.  broad:  race, gender; narrow:  science in
flat earth and round earth communities, etc.  )
•gather data
•try the code


To read
•Pia Sommerauer Master Thesis:  Conceptual Change and Distributional
Semantic Models:  an Exploratory Study on Pitfalls and Possibilities.
•Literature on Bias, Distributional Semantics, Argumentation (2nd Work-
shop on Argumentation Mining, 2015), Meaning shift.


05.02.2020
Sometimes  data  from  a  particular  subreddit  (a  group  of  discussion)  is  not
enough.  We decided to expand it, collecting all the data (max 1000 submissions
-  posts  -  due  to  API  limitations  each)  from  the  moderators  and  the  related
groups.  Now we are focusing on democrats vs republicans (very general,  can
impact on the network of concepts as a whole).  Other possible topics can be:
•creationism vs evolutionism (r/Creation vs r/evolution)
•pro and anti-capital punishment (r/abolish vs ?)
•pro-life/pro-choice (r/prolife vs r/prochoice)
•opposite attitude towards a specific person (?)
•vegan and antivegan people (r/vegan vs r/AntiVegan)
We expect more localised perturbations in the latter case, for example.

To do
•gather data from democrats and republicans
•try fasttext on it
•make a repository to share + look at the code to compare the semantic
spaces.

13.02.2020
I built two semantic spaces (with fasttext algorithm), one based on democratic
data, the other on republican one.  As a test, I checked for the correlation with
the similarity scores from MEN dataset.  The spearman correlation is very low
(0.2).
We  decided  to  perform  a  parameter  tuning  process  (Bayesian  optimization),
controlling for the hyperparameters of (1) the democrats model, (2) the repub-
licans model, (3) the concatenated model (built starting from both the corpora).

NULL  HYPOTHESIS:  there  is  no  (statistically  significant)  difference  be-
tween the optimal parameters for the three model (data is consistent).

To do
•Bayesian optimization

20.02.2020
To do
•Control for the variations in and across the models.
•Define clear hypothesis.
•Fix the optimization issues.

To read
•Conceptual Change and Distributional Semantic Models:  an Exploratory Study on Pitfalls and Possibilities
•Evaluating the consistency of word embeddings from small data

From the readings:
An informal hypothesis can be:the semantic spaces modelling two different communities  should
share  some  properties and,  at  the  same  time,  they should differ in some ways. 
I  should  find  a  method  to  be  sure  that  I  will  be  focusing  on  the real  differences in  the  spaces  and  not  the  ones  given  by random  factors impacting the act of modelling.  
I need to find a way to grasp and isolate the real shift of meaning.  Some factors that can have an impact on the research (possibly
misleading results):
• frequency effect;
• initialisation of the model and order of data given to it;
• size of the dataset
To  check  if  they  differ,  I  should  operationalise  and quantify  the  expected difference
(e.g.  through the rotation and scaling operations of (a) some  portions of the space or (b) of
some target concepts).

If we choose (b),  we can assume that the study would be similar to the ones
about  diachronic  changes  in  the  meaning  of  words.   
To  give  a  solid  methodological structure to the experiment I can:  
1.  define the process of target word selection using literature from social science, communication science, sociolinguistic or social psychology; 
2. try to differentiate between core concepts, relatedconcepts and subconcepts (related to the target words); 
3.  define control words.

I  can  select  some  target  concepts  and  look  for  their  variance  in  different corpora, compared to the (expected) lack of variance of control words.  To gain reliable results, I should compare the output of different models and check whether the expected movements/transformations of target concepts (concepts to be defined and movements to be hypothesised) are there, whether they come as we expect in all the models and so on. I should also check for the list of nearest neighbours of some target words in all the models and control for the ranking of some particular neighbours (to be
defined).

To  check  for  the  goodness  of  the  models,  before  any  check  on  the  semantic changes, I can try to compute the consistency of the dataset (which is small), using aadditive model.

01.03.2020
The best thing to do is to create a strict plan (methodologically speaking).  I
can start by the related work and then, I will write down my plan.
Hints:
• use and additive model to check for the consistency of the space.  It would be  possible  to  run  different  experiment  and  check  for  the  target  words, with different training set.
• the results should be stable.

06.03.2020
To communicate, we should share some (but not all) the beliefs we have about concepts.  A semantic space is based on the use we do of words, and, assuming that its usage is a representation of meaning, it makes sense to expect that some words’ representation should be stable across spaces and some other should not. For example, democrats and republicans will agree on the meaning of the word ’cat’ but could have a different idea of the meaning of ’immigrant’. Here  we  have  the  first  problem:  is  it  true  that  people  with  different  (political) perspectives would not share any feature of the meaning of immigrant?  It is  more  plausible  that  they  agree  on  some  (crucial)  features  and  don’t  agree on other, non-necessary features.  Indeed, they need to share some features to communicate.  Language  can  be  seen  as  a  combination  of  two  quasi-opposite
forces:  the freedom to be creative and the necessity to understand and be understood by the others.  Different perspectives concerning the meaning of each word are allowed by the freedom of being creative in language, still being bound to the necessity of communication.  Aurelie showed how scaling,  rotation and translation (at a local or global level) can model this double-faced force in the
language,  perturbating  parts  of  the  space  while  preserving  original  distances and alignment.

RESEARCH  QUESTION:  
(1)  can  these  measures  be  used  to  account  for
the difference in polarised perspective about target concepts/target areas in the
space?  
(2) Can they be used to normalise concepts that depart from their neu-
tral form?

EXPERIMENT:  Compare  semantic  spaces  that  model  different  communi-
ties (e.g.  dem, rep, neutral). 
First, select concepts or topics that are not expected to be different.  It can be
done automatically (clustering) or manually (theory-driven).  Or both.
Second, check if there is a shift in their meaning.  At a trivial level.
Third,  control  for  their  difference.   Do  they  really  differ  or  do  they  just  look
different?  Control for the impact of random factors, such as the size of the corpus, the order of data presentation, the model used and so on.
Fourth,  apply  the  vectorial  operations  to  quantify  the  difference  between  the neutral and the biased corpora.  Is there a significant difference between target and neutral concepts?
Fifth,  are  the  results  domain-dependent?   Check  for  other  hubs  of  discussion (...).  Are the results consistent?

Some problems:
• Size and quality of the corpora.  Mine are really small.  Possible solutions:
(1)  train  the  model  on  the  neutral  corpus  (or  take  pre-trained  embed-
dings), evaluate, keep training it on dem/rep, evaluate;
(2) additive model
(3) Nonce2Vec (?)
(4) if the previous ones don’t work, ask for the corpus used in Azarbonyad,
Hosein   Dehghani,  Mostafa   Beelen,  Kaspar   Arkut,  Alexandra   Marx,
Maarten   Kamps,  Jaap.   (2017).   Words  are  Malleable:  Computing  Se-
mantic Shifts in Political and Media Discourse.
• Method  to  select  concepts,  automatic  (clustering:   following  the  theory
of  prototypes  which  relies  on  the  fact  that  words  that  are  closer  to  the
cluster centroid are less likely to change (in time)), manual (theory driven
analysis,  more  stable  but  also  longer  +  needs  for  expert)  or  manual  +
automatic;
• How to define the movement that I’m expecting.  It depends on the way I
used to select the concepts.
• Errors can be due to problems at every stage.  It would be also interesting
to understand and analyse why some errors come up (or why some models
seem to provide evidence for a phenomenon and some others don’t).
• Risk of circularity.

To do
• state the possible problems and solutions in a more concrete way.
• prioritise some problems with respect to others.
• check  practically  if  it  is  possible  to  proceed  (at  list  at  the  level  of  the
dataset (first point in the list of problems).
• read broad papers, from which I can take more inspirations
